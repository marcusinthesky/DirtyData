{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARE YOU READY! üëè  I'm actually quite excited as this is a project as I have been wanting to do for a while...  Evolutionary Search over Deep Neural Network Architectures üê£.  This is actually an old concept [1](https://ieeexplore.ieee.org/document/784219/) that has had a bit of a renaissance [2](https://arxiv.org/pdf/1703.00548.pdf) üìÑ.  Cool packages such as [DEVOL](https://github.com/joeddav/devol) implement a nice version of this for 2D Convoltuion, but I really wanted to do this myself, in order to implement this for other problems like Feed Forward Neural Networks or 1d Convolution ü§ì.  [DEAP](http://deap.readthedocs.io/en/master/api/tools.html) is a popular python library for evolutionary computing and seems really versatile, so check it out.  \n",
    "  \n",
    "Ok, time to import some libraries...*boring*üìö.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from deap import base, tools, creator\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going try the classic boston houing price dataset.  üç≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Genetic Algorithms involves four main steps...  \n",
    "  \n",
    "__Initialize/Selection:__  Where we select new genes.  \n",
    "__Evalutation:__  Where we evaluate these genes.  \n",
    "__Cross-over:__  Where we 'breed' out individuals to exchange genes.  \n",
    "__Mutation:__  Where we introduce new genetic material.  \n",
    "  \n",
    "Here, we let the number of neurons and the number of layers in our network be our genes and our loss function is the loss on our hold-out set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, regulatization is really important or the network can get too big, so I included regularization on each layer's regularization for backprop and then added a regularization term for Genetic Algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalOneMax(individual, alpha=1/50):\n",
    "    inputs = Input(shape=(data.data.shape[1],))\n",
    "    \n",
    "    dense1 = Dense(abs(individual[0]), \n",
    "                   kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='relu')(inputs) if individual[0] != 0 else (inputs)\n",
    "    \n",
    "    dense2 = Dense(abs(individual[1]), \n",
    "                   kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='relu')(inputs) if individual[1] != 0 else (dense1)\n",
    "    \n",
    "    dense3 = Dense(abs(individual[2]), \n",
    "                   kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='relu')(inputs) if individual[2] != 0 else (dense2)\n",
    "    \n",
    "    output = Dense(1, kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='softmax')(inputs)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    model.compile('adam', loss=tf.nn.log_poisson_loss)\n",
    "    model.fit(data.data, data.target,\n",
    "                epochs=5,\n",
    "                batch_size=20,\n",
    "                shuffle=True,\n",
    "                validation_split=False,\n",
    "                verbose=0\n",
    "           )\n",
    "    return  (model.evaluate(data.data, data.target, batch_size=128, verbose=0).tolist() + alpha*sum(individual),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "\n",
    "N_ATTR = 3\n",
    "\n",
    "# Attribute generator \n",
    "toolbox.register(\"attr_bool\", random.randint,0,64)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, N_ATTR)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "#Evolutionary Strategies\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt,low=0, up=64, indpb=0.3)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TO EVOLVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(N_POP=5, CXPB = 0.2, MUTPB = 0.5, GEN=10):\n",
    "    \"\"\"\n",
    "    # N_POP is the population size\n",
    "    # MUTPB is the probability for mutating an individual\n",
    "    # CXPB  is the probability with which two individuals are crossed\n",
    "    # GEN is the number of generations to be run by the algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    pop = toolbox.population(N_POP)\n",
    "    \n",
    "    # Evaluate the entire population\n",
    "    fitnesses = list(map(toolbox.evaluate, pop))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit    \n",
    "    \n",
    "    # Extracting all the fitnesses of \n",
    "    fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "    # Variable keeping track of the number of generations\n",
    "    g = 0\n",
    "    \n",
    "    # Begin the evolution\n",
    "    while max(fits) < 5 and g < GEN:\n",
    "        # A new generation\n",
    "        g = g + 1\n",
    "        print(f\"-- Generation {g} --\")\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        \n",
    "        for mutant in offspring:\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "              \n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "              \n",
    "        pop[:] = offspring\n",
    "              \n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "        \n",
    "        print(f'Offspring: {offspring}')\n",
    "        print(f'Fitness Funcitons: {list(map(lambda x: round(x, 3),fits))}')\n",
    "        \n",
    "        length = len(pop)\n",
    "        mean = sum(fits) / length\n",
    "        sum2 = sum(x*x for x in fits)\n",
    "        std = abs(sum2 / length - mean**2)**0.5\n",
    "        \n",
    "        print(\"  Min %s\" % min(fits))\n",
    "        print(\"  Max %s\" % max(fits))\n",
    "        print(\"  Avg %s\" % mean)\n",
    "        print(\"  Std %s\" % std)\n",
    "        \n",
    "    return pop[fits.index(min(fits))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 11:53:07.238829 140093439174464 deprecation_wrapper.py:119] From /home/marcusskky/.conda/envs/Evolution/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0807 11:53:07.355266 140093439174464 deprecation_wrapper.py:119] From /home/marcusskky/.conda/envs/Evolution/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0807 11:53:07.376158 140093439174464 deprecation_wrapper.py:119] From /home/marcusskky/.conda/envs/Evolution/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0807 11:53:07.491070 140093439174464 deprecation_wrapper.py:119] From /home/marcusskky/.conda/envs/Evolution/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0807 11:53:07.784568 140093439174464 deprecation_wrapper.py:119] From /home/marcusskky/.conda/envs/Evolution/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0807 11:53:07.833559 140093439174464 deprecation_wrapper.py:119] From /home/marcusskky/.conda/envs/Evolution/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generation 1 --\n",
      "Offspring: [[24, 2, 14], [22, 48, 45], [21, 32, 5], [21, 47, 40], [16, 48, 45], [24, 48, 45], [10, 26, 22], [22, 2, 37], [22, 2, 14], [24, 48, 45]]\n",
      "Fitness Funcitons: [-18.515, -17.185, -18.359, -17.399, -17.288, -17.288, -18.439, -18.69, -18.69, -17.288]\n",
      "  Min -18.690363441889463\n",
      "  Max -17.185317512934386\n",
      "  Avg -17.914449157503753\n",
      "  Std 0.6333322211456874\n",
      "-- Generation 2 --\n",
      "Offspring: [[22, 2, 14], [22, 2, 14], [21, 32, 5], [27, 14, 5], [24, 2, 14], [22, 2, 14], [10, 29, 12], [10, 26, 22], [7, 47, 40], [41, 26, 22]]\n",
      "Fitness Funcitons: [-18.69, -18.69, -18.359, -18.359, -18.515, -18.69, -18.439, -18.439, -17.399, -18.439]\n",
      "  Min -18.690363441889463\n",
      "  Max -17.399129767738312\n",
      "  Avg -18.402245040893554\n",
      "  Std 0.3571849237361432\n",
      "-- Generation 3 --\n",
      "Offspring: [[22, 2, 14], [22, 2, 14], [24, 2, 47], [22, 2, 14], [22, 22, 14], [35, 2, 14], [22, 2, 14], [22, 2, 14], [22, 2, 14], [10, 29, 12]]\n",
      "Fitness Funcitons: [-18.69, -18.69, -17.979, -18.664, -18.69, -18.69, -18.69, -18.69, -18.69, -18.439]\n",
      "  Min -18.690363441889463\n",
      "  Max -17.97947271113339\n",
      "  Avg -18.59159717612587\n",
      "  Std 0.21712690137696267\n",
      "-- Generation 4 --\n",
      "Offspring: [[22, 2, 14], [24, 2, 47], [22, 22, 14], [22, 2, 14], [22, 2, 14], [35, 2, 14], [22, 22, 14], [22, 2, 14], [22, 2, 49], [22, 22, 41]]\n",
      "Fitness Funcitons: [-18.647, -18.034, -18.69, -18.69, -18.69, -18.69, -18.344, -18.711, -18.69, -18.69]\n",
      "  Min -18.710562263910948\n",
      "  Max -18.033713661616027\n",
      "  Avg -18.587693954889954\n",
      "  Std 0.21173650558233045\n",
      "-- Generation 5 --\n",
      "Offspring: [[22, 22, 14], [22, 22, 14], [22, 2, 49], [22, 2, 14], [22, 2, 14], [22, 2, 39], [22, 2, 14], [22, 2, 14], [22, 2, 14], [22, 22, 41]]\n",
      "Fitness Funcitons: [-18.69, -18.69, -18.69, -18.711, -18.711, -18.69, -18.69, -18.69, -18.69, -18.69]\n",
      "  Min -18.710562263910948\n",
      "  Max -18.690363441889463\n",
      "  Avg -18.694403206293764\n",
      "  Std 0.00807952879804052\n",
      "-- Generation 6 --\n",
      "Offspring: [[22, 2, 14], [22, 2, 14], [64, 22, 14], [22, 2, 39], [38, 52, 42], [22, 2, 14], [22, 22, 41], [22, 2, 14], [11, 2, 14], [22, 2, 14]]\n",
      "Fitness Funcitons: [-18.711, -18.711, -18.69, -18.69, -18.69, -18.69, -18.69, -18.69, -18.711, -18.711]\n",
      "  Min -18.710562263910948\n",
      "  Max -18.690363441889463\n",
      "  Avg -18.69844297069806\n",
      "  Std 0.009895361462969115\n",
      "-- Generation 7 --\n",
      "Offspring: [[11, 2, 14], [22, 22, 41], [51, 2, 14], [22, 2, 14], [11, 36, 9], [22, 12, 14], [11, 54, 14], [11, 2, 14], [22, 2, 14], [22, 1, 41]]\n",
      "Fitness Funcitons: [-18.711, -18.69, -18.15, -18.735, -18.26, -18.479, -18.711, -18.711, -18.711, -18.69]\n",
      "  Min -18.734989447914092\n",
      "  Max -18.149773079340637\n",
      "  Avg -18.584717224437732\n",
      "  Std 0.20341741775279143\n",
      "-- Generation 8 --\n",
      "Offspring: [[22, 22, 41], [22, 2, 14], [28, 2, 27], [22, 2, 46], [11, 2, 14], [22, 2, 14], [22, 2, 14], [11, 54, 14], [11, 27, 14], [22, 54, 14]]\n",
      "Fitness Funcitons: [-18.69, -18.735, -18.735, -18.735, -18.711, -18.711, -18.711, -18.711, -18.397, -17.62]\n",
      "  Min -18.734989447914092\n",
      "  Max -17.62029027599591\n",
      "  Avg -18.57544234477107\n",
      "  Std 0.33270397876109936\n",
      "-- Generation 9 --\n",
      "Offspring: [[22, 28, 26], [28, 2, 27], [22, 2, 14], [28, 37, 14], [22, 2, 38], [28, 2, 27], [22, 2, 14], [22, 2, 64], [22, 2, 14], [22, 2, 14]]\n",
      "Fitness Funcitons: [-18.735, -18.735, -18.735, -18.711, -18.711, -18.735, -18.667, -17.65, -18.65, -18.618]\n",
      "  Min -18.734989447914092\n",
      "  Max -17.649559532587705\n",
      "  Avg -18.594617164822907\n",
      "  Std 0.31744946167629917\n",
      "-- Generation 10 --\n",
      "Offspring: [[13, 2, 14], [22, 28, 26], [28, 37, 2], [22, 2, 14], [22, 2, 14], [22, 28, 26], [22, 2, 14], [22, 2, 14], [22, 2, 14], [30, 22, 27]]\n",
      "Fitness Funcitons: [-18.65, -18.735, -18.711, -18.735, -18.735, -18.735, -18.735, -18.735, -18.696, -17.782]\n",
      "  Min -18.734989447914092\n",
      "  Max -17.78196546215314\n",
      "  Avg -18.624877723588305\n",
      "  Std 0.28218583895264426\n"
     ]
    }
   ],
   "source": [
    "pop = main(N_POP=10, GEN=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome! Lets check it out.  What's really interesting is that this network has more neurons in the middle layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 28, 26]\n"
     ]
    }
   ],
   "source": [
    "print(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-19.369102990674406,)\n"
     ]
    }
   ],
   "source": [
    "loss = evalOneMax(pop, alpha=0)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
