{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARE YOU READY! üëè  I'm actually quite excited as this is a project as I have been wanting to do for a while...  Evolutionary Search over Deep Neural Network Architectures üê£.  This is actually an old concept [1](https://ieeexplore.ieee.org/document/784219/) that has had a bit of a renaissance [2](https://arxiv.org/pdf/1703.00548.pdf) üìÑ.  Cool packages such as [DEVOL](https://github.com/joeddav/devol) implement a nice version of this for 2D Convoltuion, but I really wanted to do this myself, in order to implement this for other problems like Feed Forward Neural Networks or 1d Convolution ü§ì.  [DEAP](http://deap.readthedocs.io/en/master/api/tools.html) is a popular python library for evolutionary computing and seems really versatile, so check it out.  \n",
    "  \n",
    "Ok, time to import some libraries...*boring*üìö.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from deap import base, tools, creator\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recently helped my friend Tim DeWet with a [paper](https://www.biorxiv.org/content/early/2018/06/29/358275) on the use of CRISPR. üî¨ It was very frustrating, but very fun.  I thought... no better dataset to implement a evolving neural network on than on a genetics dataset! I won't get into too much of detail of the data itself. üç≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../TimDeWet_Genomics/TrainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcussky/.conda/envs/lily/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "down = data.select(lambda col: col.startswith('Down_'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = data.logRatio\n",
    "y_transform = y_data.apply(lambda x: (-x+y_data.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Genetic Algorithms involves four main steps...  \n",
    "![Evolution](https://genetic.io/wp-content/uploads/2016/07/processus_en.png)  \n",
    "  \n",
    "__Initialize/Selection:__  Where we select new genes.  \n",
    "__Evalutation:__  Where we evaluate these genes.  \n",
    "__Cross-over:__  Where we 'breed' out individuals to exchange genes.  \n",
    "__Mutation:__  Where we introduce new genetic material.  \n",
    "  \n",
    "Here, we let the number of neurons and the number of layers in our network be our genes and our loss function is the loss on our hold-out set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, regulatization is really important or the network can get too big, so I included regularization on each layer's regularization for backprop and then added a regularization term for Genetic Algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalOneMax(individual, alpha=1/50):\n",
    "    inputs = Input(shape=(down.shape[1],))\n",
    "    \n",
    "    dense1 = Dense(abs(individual[0]), \n",
    "                   kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='relu')(inputs) if individual[0] != 0 else (inputs)\n",
    "    \n",
    "    dense2 = Dense(abs(individual[1]), \n",
    "                   kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='relu')(inputs) if individual[1] != 0 else (dense1)\n",
    "    \n",
    "    dense3 = Dense(abs(individual[2]), \n",
    "                   kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='relu')(inputs) if individual[2] != 0 else (dense2)\n",
    "    \n",
    "    output = Dense(1, kernel_regularizer=regularizers.l1_l2(0.1, 0.1), \n",
    "                   activation='softmax')(inputs)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    model.compile('adam', loss=tf.nn.log_poisson_loss)\n",
    "    model.fit(down.loc[::2,:], y_transform.loc[::2],\n",
    "                epochs=5,\n",
    "                batch_size=20,\n",
    "                shuffle=True,\n",
    "                validation_split=False,\n",
    "                verbose=0\n",
    "           )\n",
    "    return  (model.evaluate(down.loc[1::2,:], y_transform.loc[1::2], batch_size=128, verbose=0).tolist() + alpha*sum(individual),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "\n",
    "N_ATTR = 3\n",
    "\n",
    "# Attribute generator \n",
    "toolbox.register(\"attr_bool\", random.randint,0,64)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, N_ATTR)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "#Evolutionary Strategies\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt,low=0, up=64, indpb=0.3)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TO EVOLVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(N_POP=5, CXPB = 0.2, MUTPB = 0.5, GEN=10):\n",
    "    \"\"\"\n",
    "    # N_POP is the population size\n",
    "    # MUTPB is the probability for mutating an individual\n",
    "    # CXPB  is the probability with which two individuals are crossed\n",
    "    # GEN is the number of generations to be run by the algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    pop = toolbox.population(N_POP)\n",
    "    \n",
    "    # Evaluate the entire population\n",
    "    fitnesses = list(map(toolbox.evaluate, pop))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit    \n",
    "    \n",
    "    # Extracting all the fitnesses of \n",
    "    fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "    # Variable keeping track of the number of generations\n",
    "    g = 0\n",
    "    \n",
    "    # Begin the evolution\n",
    "    while max(fits) < 5 and g < GEN:\n",
    "        # A new generation\n",
    "        g = g + 1\n",
    "        print(f\"-- Generation {g} --\")\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        \n",
    "        for mutant in offspring:\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "              \n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "              \n",
    "        pop[:] = offspring\n",
    "              \n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "        \n",
    "        print(f'Offspring: {offspring}')\n",
    "        print(f'Fitness Funcitons: {list(map(lambda x: round(x, 3),fits))}')\n",
    "        \n",
    "        length = len(pop)\n",
    "        mean = sum(fits) / length\n",
    "        sum2 = sum(x*x for x in fits)\n",
    "        std = abs(sum2 / length - mean**2)**0.5\n",
    "        \n",
    "        print(\"  Min %s\" % min(fits))\n",
    "        print(\"  Max %s\" % max(fits))\n",
    "        print(\"  Avg %s\" % mean)\n",
    "        print(\"  Std %s\" % std)\n",
    "        \n",
    "    return pop[fits.index(min(fits))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generation 1 --\n",
      "Offspring: [[12, 28, 3], [4, 33, 25], [16, 54, 4], [4, 33, 3], [17, 29, 10], [4, 33, 54], [12, 28, 25], [59, 28, 4], [6, 33, 3], [4, 33, 12]]\n",
      "Fitness Funcitons: [-2.294, -1.913, -1.673, -2.353, -1.413, -2.174, -1.853, -1.334, -2.353, -2.174]\n",
      "  Min -2.3532354755401608\n",
      "  Max -1.333711835861206\n",
      "  Avg -1.9534641904830932\n",
      "  Std 0.36025956688424743\n",
      "-- Generation 2 --\n",
      "Offspring: [[4, 33, 3], [6, 33, 3], [4, 10, 25], [12, 28, 12], [6, 33, 3], [6, 33, 11], [6, 33, 3], [4, 8, 25], [4, 33, 3], [6, 33, 3]]\n",
      "Fitness Funcitons: [-2.353, -2.353, -2.374, -2.114, -2.353, -2.353, -2.314, -2.414, -2.353, -2.314]\n",
      "  Min -2.4136019248962404\n",
      "  Max -2.1135084648132323\n",
      "  Avg -2.329428523063659\n",
      "  Std 0.07680434500444087\n",
      "-- Generation 3 --\n",
      "Offspring: [[6, 33, 3], [6, 33, 3], [4, 33, 23], [4, 8, 25], [62, 8, 25], [6, 33, 3], [6, 33, 12], [4, 10, 25], [4, 8, 25], [6, 33, 3]]\n",
      "Fitness Funcitons: [-2.353, -2.353, -2.353, -2.414, -2.414, -2.353, -2.353, -2.374, -2.414, -2.353]\n",
      "  Min -2.4136019248962404\n",
      "  Max -2.3532354755401608\n",
      "  Avg -2.3733762617111203\n",
      "  Std 0.02699675475693308\n",
      "-- Generation 4 --\n",
      "Offspring: [[6, 10, 25], [4, 8, 25], [4, 8, 25], [6, 33, 3], [4, 8, 3], [6, 33, 25], [4, 8, 25], [4, 8, 25], [62, 8, 25], [4, 8, 25]]\n",
      "Fitness Funcitons: [-2.374, -2.414, -2.414, -2.353, -2.853, -1.873, -2.414, -2.414, -2.414, -2.414]\n",
      "  Min -2.853462926864624\n",
      "  Max -1.8733556385040282\n",
      "  Avg -2.3935209579467775\n",
      "  Std 0.22058106108606038\n",
      "-- Generation 5 --\n",
      "Offspring: [[4, 8, 25], [4, 8, 25], [62, 8, 3], [49, 8, 25], [4, 45, 3], [62, 8, 25], [4, 4, 25], [62, 1, 24], [6, 10, 3], [4, 8, 25]]\n",
      "Fitness Funcitons: [-2.414, -2.414, -1.693, -1.514, -2.853, -2.414, -2.414, -2.414, -2.774, -2.414]\n",
      "  Min -2.853462926864624\n",
      "  Max -1.513904954910278\n",
      "  Avg -2.3315682125091555\n",
      "  Std 0.3979975471076103\n",
      "-- Generation 6 --\n",
      "Offspring: [[4, 4, 25], [62, 1, 24], [51, 63, 3], [62, 1, 24], [4, 45, 3], [6, 10, 3], [4, 4, 25], [62, 1, 24], [4, 10, 3], [6, 45, 3]]\n",
      "Fitness Funcitons: [-2.414, -2.414, -2.774, -2.414, -2.853, -2.774, -2.414, -2.414, -2.813, -2.073]\n",
      "  Min -2.853462926864624\n",
      "  Max -2.073256694793701\n",
      "  Avg -2.535519169807434\n",
      "  Std 0.24075195421961873\n",
      "-- Generation 7 --\n",
      "Offspring: [[4, 10, 3], [4, 45, 3], [4, 10, 3], [6, 10, 3], [62, 1, 23], [62, 1, 24], [46, 63, 3], [4, 45, 3], [4, 10, 3], [3, 26, 3]]\n",
      "Fitness Funcitons: [-2.813, -2.853, -2.814, -2.773, -2.414, -2.414, -2.774, -2.853, -2.813, -2.853]\n",
      "  Min -2.853462926864624\n",
      "  Max -2.4136019248962404\n",
      "  Avg -2.7374980220794676\n",
      "  Std 0.16434148106036991\n",
      "-- Generation 8 --\n",
      "Offspring: [[6, 10, 3], [4, 45, 3], [46, 63, 3], [4, 10, 3], [4, 45, 3], [3, 26, 3], [4, 10, 3], [4, 45, 48], [3, 10, 3], [4, 26, 3]]\n",
      "Fitness Funcitons: [-2.773, -2.853, -2.774, -2.813, -2.853, -2.853, -2.813, -2.853, -2.833, -2.494]\n",
      "  Min -2.853462926864624\n",
      "  Max -2.4939838714599607\n",
      "  Avg -2.7915011930465696\n",
      "  Std 0.10356377958806542\n",
      "-- Generation 9 --\n",
      "Offspring: [[3, 10, 3], [3, 26, 3], [4, 19, 31], [4, 45, 3], [4, 54, 11], [4, 45, 3], [4, 38, 55], [3, 10, 3], [4, 26, 48], [3, 39, 3]]\n",
      "Fitness Funcitons: [-2.833, -2.853, -2.853, -2.853, -1.774, -2.114, -1.213, -2.834, -1.593, -2.254]\n",
      "  Min -2.853462926864624\n",
      "  Max -1.2132295150756836\n",
      "  Avg -2.3175674257278445\n",
      "  Std 0.5898588887759803\n",
      "-- Generation 10 --\n",
      "Offspring: [[3, 26, 3], [4, 19, 31], [60, 33, 3], [4, 45, 3], [4, 19, 31], [4, 19, 31], [3, 10, 3], [4, 45, 3], [4, 45, 3], [4, 45, 3]]\n",
      "Fitness Funcitons: [-2.853, -2.853, -2.853, -2.853, -2.853, -2.853, -2.833, -2.853, -2.853, -2.853]\n",
      "  Min -2.853462926864624\n",
      "  Max -2.8334798545837403\n",
      "  Avg -2.851464619636535\n",
      "  Std 0.005994921684565828\n"
     ]
    }
   ],
   "source": [
    "pop = main(N_POP=10, GEN=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome! Lets check it out.  What's really interesting is that this network as very few neurons in early and late layers.  That's not something I would have immediately experimented with, as much of the literture suggests the need for many neurons in early layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 26, 3]\n"
     ]
    }
   ],
   "source": [
    "print(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-3.152746955871582,)\n"
     ]
    }
   ],
   "source": [
    "loss = evalOneMax(pop, alpha=0)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
