{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appologies to the reader: there is a lot of code in this post, but if you stay til the end there is a cookie emoji for you! ‚ûó The City of Cape Town have a great initialive for [Open Data](http://web1.capetown.gov.za/web1/opendataportal/default), with huge volumns of data of rental price, wifi usage and service delivery requests. üíΩ This data is extensive, but has- as extected- many challenges to visualising and understanding the data itself.  üìä\n",
    "  \n",
    "Let's import some libraries!üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from multiprocessing import pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils.validation import check_symmetric\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cluster import FeatureAgglomeration, AffinityPropagation, DBSCAN\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.manifold import TSNE,smacof, MDS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "from gensim.models.deprecated.old_saveload import SaveLoad\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has some weird formatting, and a number of the files are either corrupt or formatted as xls and xlsx file types. üìÅ Any-who, I import a ransom rample of them to play around with a see what I could find.  üòï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service requests September 2017.xlsx\n",
      "Service requests October 2017.xlsx\n",
      "Service requests August 2017.xlsx\n",
      "Service requests December 2017.xlsx\n",
      "Service requests August 2016.xlsx\n",
      "Service requests July 2017.xlsx\n",
      "Service requests November 2017.xlsx\n"
     ]
    }
   ],
   "source": [
    "parent = os.path.join('Data','CapeTownServiceCalls')\n",
    "data = pd.DataFrame(None)\n",
    "\n",
    "for file in os.listdir(parent):\n",
    "    if file.endswith(\".xlsx\") & (os.path.getsize(os.path.join(parent, file))!=0):\n",
    "        child = os.path.join(parent, file)\n",
    "        load = pd.read_excel(child, na_values=['Not assigned','','nan','#'])\n",
    "        load.columns = load.iloc[3,:]\n",
    "        load = load.drop([0,1,2,3], axis=0).dropna()\n",
    "        load['file'] = file\n",
    "        \n",
    "        data = pd.concat([data,load], axis=0, sort=False)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you will notice in the dataset, is that the data is insane! üòÆ It's medium-size, but has no clear standardization. üêò Fields like description and suburb allow users to fill in whatever they want and users are terrible at spelling! ‚úÖ The data has this strange formatting for coordinated, which isn't in normal longitude and lattitude, but is also either massively positive or massively negative and has some unknown origin... This I think was some annonymization so I didn't too hard to try deanonymize it on a public blog.  üó∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461301, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = data.columns.tolist()\n",
    "columns[6] = 'Description'\n",
    "data.columns = columns\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sub Council</th>\n",
       "      <th>Ward</th>\n",
       "      <th>Suburb</th>\n",
       "      <th>C3 Complaint Type</th>\n",
       "      <th>Work Center</th>\n",
       "      <th>Notification</th>\n",
       "      <th>Description</th>\n",
       "      <th>X-Y Co ordinate 1</th>\n",
       "      <th>X-Y Co ordinate 2</th>\n",
       "      <th>Created On Date</th>\n",
       "      <th>Notifications Created</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32904</th>\n",
       "      <td>Sub-Council 2</td>\n",
       "      <td>102</td>\n",
       "      <td>VREDEKLOOF</td>\n",
       "      <td>Internal : Billing  / Adjustment Queries</td>\n",
       "      <td>Scottsdene: Water</td>\n",
       "      <td>1012539430</td>\n",
       "      <td>Internal : Billing  / Adjustment Queries</td>\n",
       "      <td>30142.140000</td>\n",
       "      <td>3748999.620000</td>\n",
       "      <td>2017-08-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Service requests August 2017.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23561</th>\n",
       "      <td>Sub-Council 17</td>\n",
       "      <td>048</td>\n",
       "      <td>BELTHORN ESTATE</td>\n",
       "      <td>4B: Investigation</td>\n",
       "      <td>Water Restrictions 4B CAPE TOWN</td>\n",
       "      <td>1012989580</td>\n",
       "      <td>4B Contravention</td>\n",
       "      <td>44607.960000</td>\n",
       "      <td>3761880.400000</td>\n",
       "      <td>2017-12-11 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Service requests December 2017.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58902</th>\n",
       "      <td>Sub-Council 5</td>\n",
       "      <td>031</td>\n",
       "      <td>BONTEHEUWEL</td>\n",
       "      <td>Carpentry</td>\n",
       "      <td>Housing - Bishop Lavis: Carpentry</td>\n",
       "      <td>1012738396</td>\n",
       "      <td>Lounge window frame and handle loose</td>\n",
       "      <td>-40424.6883337413</td>\n",
       "      <td>-3759202.43666664</td>\n",
       "      <td>2017-10-03 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Service requests October 2017.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25076</th>\n",
       "      <td>Sub-Council 16</td>\n",
       "      <td>077</td>\n",
       "      <td>GARDENS</td>\n",
       "      <td>PPM Customer Unable to Purchase</td>\n",
       "      <td>Electricity Generation &amp; Distribution</td>\n",
       "      <td>1012864900</td>\n",
       "      <td>1012864900</td>\n",
       "      <td>54845.080000</td>\n",
       "      <td>3756384.310000</td>\n",
       "      <td>2017-11-06 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Service requests November 2017.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67385</th>\n",
       "      <td>Sub-Council 8</td>\n",
       "      <td>086</td>\n",
       "      <td>LWANDLE</td>\n",
       "      <td>PPM Faulty (Enter detailed description)</td>\n",
       "      <td>Electricity-First Line Response_Helderbe</td>\n",
       "      <td>1011405110</td>\n",
       "      <td>PPM Faulty 21798 VULINDLELA ROAD GIFT</td>\n",
       "      <td>-12394.6996</td>\n",
       "      <td>-3777127.1098</td>\n",
       "      <td>2016-08-14 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Service requests August 2016.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30915</th>\n",
       "      <td>Sub-Council 19</td>\n",
       "      <td>067</td>\n",
       "      <td>SEAWINDS</td>\n",
       "      <td>No Water WMD</td>\n",
       "      <td>Cape Town / SPM</td>\n",
       "      <td>1011413323</td>\n",
       "      <td>No Water WMD</td>\n",
       "      <td>-47649.9665</td>\n",
       "      <td>-3772489.5394</td>\n",
       "      <td>2016-08-16 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Service requests August 2016.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70665</th>\n",
       "      <td>Sub-Council 7</td>\n",
       "      <td>103</td>\n",
       "      <td>BONNIE BROOK</td>\n",
       "      <td>High Water Pressure</td>\n",
       "      <td>Scottsdene: Water</td>\n",
       "      <td>1012942094</td>\n",
       "      <td>High Water Pressure</td>\n",
       "      <td>27816.280000</td>\n",
       "      <td>3745484.380000</td>\n",
       "      <td>2017-11-27 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Service requests November 2017.xlsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sub Council Ward           Suburb  \\\n",
       "32904   Sub-Council 2  102       VREDEKLOOF   \n",
       "23561  Sub-Council 17  048  BELTHORN ESTATE   \n",
       "58902   Sub-Council 5  031      BONTEHEUWEL   \n",
       "25076  Sub-Council 16  077          GARDENS   \n",
       "67385   Sub-Council 8  086          LWANDLE   \n",
       "30915  Sub-Council 19  067         SEAWINDS   \n",
       "70665   Sub-Council 7  103     BONNIE BROOK   \n",
       "\n",
       "                              C3 Complaint Type  \\\n",
       "32904  Internal : Billing  / Adjustment Queries   \n",
       "23561                         4B: Investigation   \n",
       "58902                                 Carpentry   \n",
       "25076           PPM Customer Unable to Purchase   \n",
       "67385   PPM Faulty (Enter detailed description)   \n",
       "30915                              No Water WMD   \n",
       "70665                       High Water Pressure   \n",
       "\n",
       "                                    Work Center Notification  \\\n",
       "32904                         Scottsdene: Water   1012539430   \n",
       "23561           Water Restrictions 4B CAPE TOWN   1012989580   \n",
       "58902         Housing - Bishop Lavis: Carpentry   1012738396   \n",
       "25076     Electricity Generation & Distribution   1012864900   \n",
       "67385  Electricity-First Line Response_Helderbe   1011405110   \n",
       "30915                           Cape Town / SPM   1011413323   \n",
       "70665                         Scottsdene: Water   1012942094   \n",
       "\n",
       "                                    Description     X-Y Co ordinate 1  \\\n",
       "32904  Internal : Billing  / Adjustment Queries          30142.140000   \n",
       "23561                          4B Contravention          44607.960000   \n",
       "58902      Lounge window frame and handle loose     -40424.6883337413   \n",
       "25076                                1012864900          54845.080000   \n",
       "67385     PPM Faulty 21798 VULINDLELA ROAD GIFT           -12394.6996   \n",
       "30915                              No Water WMD           -47649.9665   \n",
       "70665                       High Water Pressure          27816.280000   \n",
       "\n",
       "          X-Y Co ordinate 2      Created On Date Notifications Created  \\\n",
       "32904        3748999.620000  2017-08-02 00:00:00                     1   \n",
       "23561        3761880.400000  2017-12-11 00:00:00                     1   \n",
       "58902     -3759202.43666664  2017-10-03 00:00:00                     1   \n",
       "25076        3756384.310000  2017-11-06 00:00:00                     1   \n",
       "67385         -3777127.1098  2016-08-14 00:00:00                     1   \n",
       "30915         -3772489.5394  2016-08-16 00:00:00                     1   \n",
       "70665        3745484.380000  2017-11-27 00:00:00                     1   \n",
       "\n",
       "                                      file  \n",
       "32904    Service requests August 2017.xlsx  \n",
       "23561  Service requests December 2017.xlsx  \n",
       "58902   Service requests October 2017.xlsx  \n",
       "25076  Service requests November 2017.xlsx  \n",
       "67385    Service requests August 2016.xlsx  \n",
       "30915    Service requests August 2016.xlsx  \n",
       "70665  Service requests November 2017.xlsx  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sub Council                  26\n",
       "Ward                        217\n",
       "Suburb                     5108\n",
       "C3 Complaint Type           557\n",
       "Work Center                 979\n",
       "Notification             461301\n",
       "Description              159969\n",
       "X-Y Co ordinate 1        296441\n",
       "X-Y Co ordinate 2        294635\n",
       "Created On Date             214\n",
       "Notifications Created         2\n",
       "file                          7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Problem: Suburb Descriptions\n",
    "As it turns out, they do not have a drop-down for suburb descriptions, so people just put down what they want and make a lot of spelling mistakes. üé´ There are only a hand-full on suburbs in Cape Town but there are over 3300 labels, most of which are tiny one-offs.  üó≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3359,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Suburb = data.Suburb.str.lower().str.replace('[^a-z\\s]', ' ').str.split().str.join(' ').str.lstrip(' ').str.rstrip(' ')\n",
    "labels = data.Suburb.value_counts()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic method to calculate the distance between words or phrases in some spellcheck is using the Levenshtein Distance, but this is a computationally intensive distance metric, which despite my best efforts parallelizing the code, is stupidly slow to compute. ü§ñ So what we are going to try is to compute the tdidf of the charcaters, then calculate the cosine similarity of the phrases.  Because there are so many entries, we are going to try cheating and rather than computing the distance for all points, we are just going to do unique points, and make a adjustmnets in order to approximate from density based method, by adjusting for the number of entries with that label, as well and the length of the descriptions. üóú  This is much much much faster to compute, so I'm going to be as lazy as possible here! üèá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3359, 3359)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='char')\n",
    "\n",
    "char_vectors = tfidf.fit_transform(labels.index)\n",
    "char_distances = pairwise_distances(char_vectors, metric='euclidean', n_jobs=-1)\n",
    "\n",
    "size_regulaization = pd.Series(labels.values)\n",
    "size_regulaization = np.log(size_regulaization)\n",
    "size_regulaization = (size_regulaization/(size_regulaization.var()**1.5))+1\n",
    "\n",
    "size_weighted_distances = pd.DataFrame(np.transpose(char_distances/size_regulaization.values.reshape(-1,1)), columns=labels.index, index=labels.index)\n",
    "\n",
    "length = pd.Series(labels.index).apply(lambda x: len(x))\n",
    "length_regularize = length.values.reshape(-1,1)+(np.transpose(length.values.reshape(-1,1)))\n",
    "length_regularize = length_regularize - length_regularize.mean()\n",
    "length_regularize = length_regularize/(length_regularize.var())\n",
    "length_regularize = length_regularize + 1\n",
    "\n",
    "#Arithmetic Mean\n",
    "#length_regularize = length.values.reshape(-1,1).dot(np.transpose(length.values.reshape(-1,1)))**0.5\n",
    "\n",
    "sizelength_weighted_distances = pd.DataFrame(size_weighted_distances/length_regularize)\n",
    "sizelength_weighted_distances.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcussky/.conda/envs/tulip/lib/python3.6/site-packages/sklearn/utils/validation.py:709: UserWarning:\n",
      "\n",
      "Array is not symmetric, and will be converted to symmetric by average with its transpose.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mds = TSNE(n_components=2, metric='precomputed')\n",
    "sizelength_weighted_distances = check_symmetric(sizelength_weighted_distances.astype(np.float64))\n",
    "mds_transform = mds.fit_transform(sizelength_weighted_distances.astype(np.float64).values)\n",
    "mds_transform = pd.DataFrame(mds_transform, columns=['x','y'])\n",
    "word_clustering = FeatureAgglomeration(n_clusters=2000, affinity='precomputed', memory=None, connectivity=None, compute_full_tree=True, linkage='complete')\n",
    "clusters = pd.DataFrame(word_clustering.fit_transform(-sizelength_weighted_distances))\n",
    "clusters = clusters.idxmax(1)\n",
    "clusters.index = labels.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets try visualize these weight spellings and groups we can form from them. üñ• I know this looks hard to visualize here, but the main here is not to do clustering, but spell-check so we want to be as gentle as posssible.  I make the size indicate the size of the surburb tag in the corpus.  üíØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~marcussky/43.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_notebook_mode(connected=True)\n",
    "py.iplot([go.Scatter(x=mds_transform.x.tolist(), y=mds_transform.y.tolist(),  mode = 'markers', text=(clusters.apply(lambda x: str(x) + ': ') + labels.index).values.tolist(), \n",
    "                 marker=dict(\n",
    "                     size=(size_regulaization**4+3).tolist(),\n",
    "                     color=clusters.tolist(),\n",
    "                     colorscale='Viridis',\n",
    "                 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f38197fd470>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(clusters.value_counts()).plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here at least for the big clusters, we can see it has worked pretty well.  Not perfect, I admit, but pretty well considering how quick it was.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "khayelitsha           1383\n",
       "khayelisha            1383\n",
       "khayalitsha           1383\n",
       "khayalitcha           1383\n",
       "khayelitsa            1383\n",
       "kahyelitsha           1383\n",
       "khayelithsa           1383\n",
       "khaylitsha            1383\n",
       "khayeltsha            1383\n",
       "siyahlala             1383\n",
       "khayekitsha           1383\n",
       "khkayelistha          1383\n",
       "khaylistha            1383\n",
       "kayelitsha            1383\n",
       "highlay               1383\n",
       "khayaletisha          1383\n",
       "kihayelitsha          1383\n",
       "khayalitha            1383\n",
       "eyethu khayelitsha    1383\n",
       "khayelistha           1383\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.loc[clusters == clusters.value_counts().idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.concat([clusters, labels], axis=1, sort=False)\n",
    "clusters.columns = ['cluster', 'count']\n",
    "dictionary = pd.merge(clusters,clusters.groupby('cluster').idxmax(), how='left', on='cluster')\n",
    "dictionary['old_name'] = clusters.index\n",
    "dictionary.index = clusters.index\n",
    "dictionary.columns = ['cluster','count','new_name','old_name']\n",
    "\n",
    "data = pd.merge(data, dictionary, how='left', left_on='Suburb', right_on='old_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look this could also be improved by doing clustering on words, then on sentences, but the issue comes in words which are randomly split.  So at somepoints you are going to have to do this accross the whole phrase, you must just decide when.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Problem: Descriptions! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a lot of descriptions, just a lot, as we want to check out themes these is a challenge as we need to figure out how to gorup them.  Now, a common technique for handling text and topics is a technique called Latent Drichlet Allocation- but I tried it and it wasn't the best at all.  This is for a couple of reasons üî¢:  \n",
    "1.  The descriptions are really short, so don't have enough words to make it stable.  \n",
    "2.  It would require me hand labelling the groupings and I'm very lazy when it comes to labelling.  \n",
    "  \n",
    "So how do we cheat.  This is the plan:  \n",
    "0Ô∏è‚É£ Clean the text, remove the stop_words like 'the'  \n",
    "1Ô∏è‚É£ Use word2vec to get word vectors  \n",
    "2Ô∏è‚É£ Take a weighted average of the word vectors to get document vectors  \n",
    "3Ô∏è‚É£ Use those to do clustering on topics or concepts  \n",
    "4Ô∏è‚É£ Take a weighted average of the clusters to get a topics vector  \n",
    "5Ô∏è‚É£ Use that topic vector to get a word vector nearest to it  \n",
    "6Ô∏è‚É£ Visualize the whole thing for your lovely eyes to see  \n",
    "  \n",
    "Ready....Go! üèå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sewer: Blocked/Overflow                 68386\n",
       "No Power                                32447\n",
       "No Water WMD                            31310\n",
       "Customer : Reconnection Request         16010\n",
       "4B: Investigation                       11816\n",
       "Leak at Water Meter / Stopcock          11323\n",
       "WMD: Meter: Aqualoc Fault               10956\n",
       "Customer : Meter Reading/Consumption     9303\n",
       "No Water Supply                          8236\n",
       "Stolen Bins - 240L                       8103\n",
       "Name: C3 Complaint Type, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = data.loc[:,'C3 Complaint Type'].value_counts()\n",
    "vc.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vc.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStops(x, stops=None):\n",
    "    \"\"\"Removes stopwords from sentences\n",
    "    \n",
    "    Libraries & Versions:\n",
    "    Python==3.6.5\n",
    "    Pandas=='0.23.1' as pd\n",
    "    nltk=='3.3'\n",
    "    \n",
    "    Keyword arguments:\n",
    "    X -- Pandas Series of text as strings\n",
    "    stops -- own stopwords provided\n",
    "    \"\"\"\n",
    "    stop = list(stop_words.ENGLISH_STOP_WORDS) + ['may',\n",
    "                                                  'also','zero','one','two',\n",
    "                                                  'three','four','five','six','seven',\n",
    "                                                  'eight','nine','ten','eleven','twelve',\n",
    "                                                  'thirteen','fourteen','fifteen',\n",
    "                                                  'sixteen','seventeen','eighteen','nineteen',\n",
    "                                                  'twenty','thiry','forty','fifty','sixty',\n",
    "                                                  'seventy','eighty','ninety','hundred',\n",
    "                                                  'thousand','million','billion','trillion',\n",
    "                                                  'accross','amoung','among','beside','within',\n",
    "                                                  'company', 'holdings','inc','incorperated',\n",
    "                                                  'group','limited','plc', 'companies',\n",
    "                                                  'listings', 'list','of', 'stock',\n",
    "                                                  'exchange']+list(string.ascii_lowercase)\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    x = x.str.replace(pat, '')\n",
    "    x = x.str.replace(r'\\s+', ' ')\n",
    "           \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedWordVectors(TransformerMixin):\n",
    "    \"\"\"Trains Collation Model to join co-occuring works into single entities or concepts\n",
    "    \n",
    "    Libraries & Versions:\n",
    "    Python==3.6.5\n",
    "    Pandas=='0.23.1' as pd\n",
    "    nltk=='3.3'\n",
    "    numpy=='1.14.5'\n",
    "    \n",
    "    Keyword arguments:\n",
    "    X -- Pandas Series of text as strings\n",
    "    \"\"\"\n",
    "    def __init__(self, model=None):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Train word vectors\n",
    "        self.word2vec = Word2Vec(sentences=X.str.split(' '), min_count=1, size=50, workers=cpu_count()-3, iter=2)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #Fit TFIDF\n",
    "        self.tfidf = TfidfVectorizer(vocabulary= list(self.word2vec.wv.vocab.keys()))\n",
    "        idfs = self.tfidf.fit_transform(X)\n",
    "        inverse_idfs = self.tfidf.inverse_transform(idfs)\n",
    "        \n",
    "        #Weighted Vectors\n",
    "        weighted_docs = []        \n",
    "        for idf, inv in zip(idfs, inverse_idfs):\n",
    "            try:\n",
    "                weight = idf[idf!=0]\n",
    "                vector = self.word2vec[inv]\n",
    "                weighted_doc = weight.dot(vector)\n",
    "            except:\n",
    "                weighted_doc = np.empty((1, self.word2vec.wv.vector_size), float)    \n",
    "            weighted_docs.append(weighted_doc.tolist()[0])\n",
    "                \n",
    "        return pd.DataFrame(weighted_docs).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcussky/.conda/envs/tulip/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(557, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    FunctionTransformer(func=lambda x: x.str.lower(), validate=False),\n",
    "    FunctionTransformer(func=lambda x: x.str.replace('[^a-zA-Z]', ' '), validate=False),\n",
    "    FunctionTransformer(func=RemoveStops, validate=False),\n",
    "    WeightedWordVectors()\n",
    ")\n",
    "\n",
    "Description_Vectors = pipe.fit_transform(data.loc[:,'C3 Complaint Type'])\n",
    "unique_descriptions = vc.index.str.lower().str.replace('[^a-z\\s]', ' ').str.split().str.join(' ').str.lstrip(' ').str.rstrip(' ')\n",
    "uniques_descriptions = pd.Series(unique_descriptions).loc[unique_descriptions!='']\n",
    "Unique_Description_Vectors = pipe.transform(unique_descriptions)\n",
    "Unique_Description_Vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discription_tsne = TSNE(n_components=2)\n",
    "dist_comp = discription_tsne.fit_transform(Unique_Description_Vectors)\n",
    "dist_comp = pd.DataFrame(dist_comp, columns=['x','y'])\n",
    "\n",
    "clustering_pipeline = make_pipeline(\n",
    "    FunctionTransformer(lambda x: pairwise_distances(x, metric='cosine')),\n",
    "    AffinityPropagation(damping=0.5, max_iter=1000, convergence_iter=400,affinity='precomputed', preference=vc.values/(547)),\n",
    ")\n",
    "discription_clusters = pd.Series(clustering_pipeline.fit_predict(dist_comp))\n",
    "discription_clusters.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~marcussky/45.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_notebook_mode(connected=True)\n",
    "py.iplot([go.Scatter(x=dist_comp.x.tolist(), y=dist_comp.y.tolist(),  mode = 'markers', text=(discription_clusters.apply(lambda x: str(x) + ': ') + uniques_descriptions).tolist(), \n",
    "                 marker=dict(\n",
    "                     size=vc.apply(lambda x: np.log(x)*2).tolist(),\n",
    "                     color=discription_clusters.tolist(),\n",
    "                     colorscale='Viridis',\n",
    "                 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcussky/.conda/envs/tulip/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "discription_clusters = pd.concat([discription_clusters, pd.Series(vc.index.values), pd.Series(vc.values)], axis=1, sort=False)\n",
    "discription_clusters.columns = ['cluster','name', 'count']\n",
    "description_dictionary = pd.merge(discription_clusters,discription_clusters.groupby('cluster').max(), how='left', on='cluster')\n",
    "description_dictionary.columns = ['cluster','old_description','count_x','new_description','count_y']\n",
    "\n",
    "cluster_word = pd.DataFrame()\n",
    "for i in range(description_dictionary.cluster.nunique()):\n",
    "    cluster_mean = pd.DataFrame()\n",
    "    cluster_size = 0\n",
    "    for i in description_dictionary.loc[description_dictionary.cluster == i,:].index:\n",
    "        cluster_mean = pd.concat([cluster_mean, pd.DataFrame(Unique_Description_Vectors.loc[i,:] * (description_dictionary.loc[i,'count_x'])).transpose()], axis=0)\n",
    "        cluster_size = cluster_size + (description_dictionary.loc[i,'count_x'])\n",
    "    cluster_word = pd.concat([cluster_word,pd.DataFrame(cluster_mean.sum()/cluster_size).transpose()], axis=0)\n",
    "\n",
    "cluster_word = cluster_word.reset_index()\n",
    "cluster_word = cluster_word.drop(columns='index')\n",
    "\n",
    "words = []\n",
    "for word in range(0, cluster_word.shape[0]):\n",
    "    words.append(pipe.named_steps.weightedwordvectors.word2vec.most_similar(positive=[cluster_word.iloc[word,:].values], topn=1)[0])\n",
    "words = pd.DataFrame(words, columns=['word','similarity']).reset_index()\n",
    "\n",
    "description_dictionary = pd.merge(description_dictionary, words, how='left', left_on='cluster', right_on='index')\n",
    "\n",
    "data = pd.merge(data, description_dictionary, how='left', left_on='C3 Complaint Type', right_on='old_description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it together: Sankey Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I recently discovered one of the coolest plots and [visualization of soy production is South Africa](https://trase.earth/flows).  Check it out! üõé They use a plot called a sankey plot which shows flows or relationships between equal sets.  Being lazy, I thought why dont I just get these relationships using a one-hot encoding and then OLS regression then times the coefficients but the size of the data to get weighted edges.  üìè  It was a great hack and I recommend it to those wanting to enjoy more Sankey Plots in their lives! üñº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_complaints = data.loc[:,'word'].value_counts().nlargest(n=10)\n",
    "largest_suburbs = data.loc[:,'new_name'].value_counts().nlargest(n=5)\n",
    "largest_centres = data.loc[:,'Work Center'].value_counts().nlargest(n=5)\n",
    "\n",
    "sankey_sample = data.loc[(data.loc[:,'word'].isin(largest_complaints.index.tolist()))&\n",
    "                          (data.loc[:,'new_name'].isin(largest_suburbs.index.tolist()))&\n",
    "                          (data.loc[:,'Work Center'].isin(largest_centres.index.tolist())),:]\n",
    "\n",
    "sparse_complaint = pd.get_dummies(sankey_sample.loc[:,'word'])\n",
    "sparse_suburb = pd.get_dummies(sankey_sample.new_name)\n",
    "sparse_centres = pd.get_dummies(sankey_sample.loc[:,'Work Center'])\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(sparse_complaint, sparse_suburb)\n",
    "connections = lr.coef_*sparse_suburb.sum().values.reshape(-1,1).dot(np.transpose(sparse_complaint.sum().values.reshape(-1,1)))/sparse_suburb.sum().sum()\n",
    "\n",
    "lr2 = LinearRegression(fit_intercept=False)\n",
    "lr2.fit(sparse_centres, sparse_complaint)\n",
    "connections2 = lr2.coef_*sparse_complaint.sum().values.reshape(-1,1).dot(np.transpose(sparse_centres.sum().values.reshape(-1,1)))/sparse_complaint.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~marcussky/47.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sankey_data = dict(\n",
    "    type='sankey',\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(\n",
    "        color = \"black\",\n",
    "        width = 0.5\n",
    "      ),\n",
    "      label = sparse_suburb.columns.tolist()+sparse_complaint.columns.tolist()+sparse_centres.columns.tolist()\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = lr.coef_.nonzero()[0].tolist()+[i+sparse_suburb.shape[1] for i in lr2.coef_.nonzero()[0].tolist()],\n",
    "      target = [i+sparse_suburb.shape[1] for i in lr.coef_.nonzero()[1].tolist()]+[i+sparse_suburb.shape[1]+sparse_complaint.shape[1] for i in lr2.coef_.nonzero()[1].tolist()],\n",
    "      value = connections[lr.coef_.nonzero()[0].tolist(),lr.coef_.nonzero()[1].tolist()].tolist()+connections2[lr2.coef_.nonzero()[0].tolist(),lr2.coef_.nonzero()[1].tolist()].tolist()\n",
    "  ))\n",
    "\n",
    "layout =  dict(\n",
    "    title = \"Service Delivery Flow\",\n",
    "    font = dict(\n",
    "      size = 10\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=[sankey_data], layout=layout)\n",
    "py.iplot(fig, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here blocked is got to do with issues with public toilets from the data.  One series issue seems to be water, power, sanitation and theft. üï∂ This is no real suprise, but hopefully for dashboards like this can really expand the understrand of issues in our city! üåÉ Thanks for staying. Cookie Emoji for you üç™.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
